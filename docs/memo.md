# Execution‑Grounded Kernel Optimization with Coupled Train‑Time RLVR and Test‑Time Self‑Learning

Modern AI systems are bottlenecked by the quality of their low‑level kernels, and kernel engineering remains expert‑heavy and hard to scale. Recent work on LLM‑based kernel generation emphasizes iterative, feedback‑driven optimization loops and highlights a fragmented landscape of datasets, benchmarks, and agentic workflows. In this context, our goal is to deliver a clean, execution‑grounded environment that makes self‑learning measurable and reproducible, while remaining simple enough to audit and extend.

This release introduces a KernelBench‑based reinforcement learning environment designed to couple two learning loops through a single, deterministic evaluator. The outer loop is train‑time RLVR, intended to build a policy that generalizes across tasks. The inner loop is test‑time adaptation via low‑rank weight updates on a per‑task basis, intended to show explicit self‑improvement when the model is already competent. Both loops are grounded in the same execution feedback: the environment compiles, runs, and benchmarks candidate kernels and assigns reward based on correctness and speedup. The core claim of this system is not that any single training run achieves state‑of‑the‑art performance, but that a single execution‑grounded interface can stably drive both learning regimes and make their differences observable under equal rollout budgets.

The environment is deliberately scoped for rigor and reproducibility. Each task provides a prompt and a reference implementation; the model outputs only the body of ModelNew.forward, and the environment deterministically scaffolds the full program, then performs correctness checks and timing using the KernelBench harness. This “scaffolded action space” removes format errors without introducing heuristic reward shaping. The reward function is execution‑grounded speedup under correctness gating, and all artifacts—raw action, assembled code, compile logs, runtime statistics, and reward—are logged for auditing and replay.

Our training configuration uses on‑policy GRPO‑style rollouts with group sampling and streaming minibatches, paired with LoRA fine‑tuning. We train a single base policy on KernelBench L1 to validate generalization across tasks, then evaluate the same checkpoint under three regimes: base policy, best‑of‑N sampling with equal rollout budget, and test‑time LoRA adaptation with the same budget. This comparison is designed to test the specific self‑learning hypothesis: that per‑task weight updates outperform pure search under equal compute.

Results (pending). The following section will be updated with the final empirical evidence. For the L1 evaluation split, we will report fast_1, correctness rate, and reward distributions for the base policy, best‑of‑N (K=64), and inner‑loop adaptation (K=64, steps=15). These results will be inserted here: [RESULTS TBD]. We will also provide full training curves and checkpoint metadata to allow independent verification: [RESULTS TBD].

Related work motivates both the environment design and the evaluation strategy. Execution‑grounded AI research emphasizes that automated execution is feasible at scale and that evolutionary search can outperform RL baselines on upper‑bound metrics, while naïve RL can improve average reward yet collapse diversity and fail to improve the best solutions. This informs our insistence on comparing best‑of‑N and test‑time RL under equal budgets rather than reporting only average reward. Recent work on self‑teaching (SOAR) frames self‑improvement as a grounded teacher‑student process and argues that grounding rewards in measurable progress avoids collapse modes common to intrinsic‑reward self‑play; this reinforces our choice to keep reward strictly tied to execution rather than intrinsic proxies. (https://arxiv.org/pdf/2601.18778) Dr. Zero’s self‑evolving agents and its hop‑grouped relative policy optimization highlight how grouping can reduce sampling overhead while maintaining stability; this supports our use of group‑based rollouts and motivates future extensions that scale curriculum generation without undermining execution grounding. (https://arxiv.org/abs/2601.07055) The broader kernel‑generation survey underscores that the field is expanding rapidly across datasets and benchmarks, and that robust, verifiable environments are the common missing substrate for comparative research.

Limitations are explicit. We currently focus on KernelBench L1, which is the easiest tier, and thus do not claim generalization to higher‑difficulty kernels. Our rollout budget is smaller than single‑task test‑time training setups, and we have not yet introduced curriculum generation or multi‑task self‑evolution. If the step‑10 checkpoint shows no uplift, we will apply the smallest possible signal‑amplifying changes—per‑task normalization, proxy evaluation during training with full evaluation at checkpoints, and a minimal correctness bonus—while preserving the execution‑grounded nature of the reward. The aim is to hold ourselves to the same standard we want in production: a clear, falsifiable narrative backed by rigorous, reproducible evidence.

In summary, this environment is designed as a release‑quality artifact that demonstrates a tightly coupled train‑time and test‑time learning loop over real kernel tasks. Its success is not defined by hype but by the clarity of the evidence it produces and the ease with which other researchers can verify and extend it.
